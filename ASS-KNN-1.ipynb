{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd473a0-6b3d-4b55-b234-6720088c5c64",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It makes predictions based on the majority class or the average of the k-nearest data points in the feature space. In classification, it assigns a class label to a new data point based on the class labels of its nearest neighbors, while in regression, it predicts a continuous value based on the average of the values of its k-nearest neighbors.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The choice of the value of K in KNN is a crucial hyperparameter. A smaller K can lead to noisy predictions, while a larger K can lead to overly smooth predictions. You can choose K through techniques like cross-validation, grid search, or by using domain knowledge. Generally, odd values for K are preferred to avoid ties in class assignments.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The primary difference is in their output:\n",
    "\n",
    "KNN Classifier: It predicts the class label of a data point based on the majority class among its k-nearest neighbors.\n",
    "KNN Regressor: It predicts a continuous value for a data point based on the average or weighted average of the values of its k-nearest neighbors.\n",
    "Q4. How do you measure the performance of KNN?\n",
    "Common performance metrics for KNN include accuracy, precision, recall, F1-score, and ROC curves for classification tasks, and metrics like mean squared error (MSE), R-squared, and mean absolute error (MAE) for regression tasks. The choice of metric depends on the specific problem.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The curse of dimensionality in KNN refers to the fact that as the number of features or dimensions in the data increases, the distance between data points becomes less meaningful. In high-dimensional spaces, all data points appear to be equally distant, which can negatively impact the performance of KNN. Techniques like dimensionality reduction or feature selection can help mitigate this issue.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Handling missing values in KNN can be challenging. You can either impute missing values before applying KNN or use techniques like mean imputation or k-Nearest Neighbors imputation to fill in missing values based on nearby data points.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The choice between KNN classifier and regressor depends on the problem at hand:\n",
    "\n",
    "KNN Classifier: Suitable for problems where the output is categorical, such as image classification or spam detection.\n",
    "KNN Regressor: Appropriate for problems where the output is continuous, like predicting housing prices or stock prices.\n",
    "\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Simple to understand and implement.\n",
    "Non-parametric, meaning it can capture complex relationships.\n",
    "Works well for small to moderately sized datasets.\n",
    "Weaknesses:\n",
    "\n",
    "Sensitive to the choice of K.\n",
    "Computationally expensive for large datasets.\n",
    "Suffers from the curse of dimensionality.\n",
    "To address these issues, you can use techniques like cross-validation for K selection, dimensionality reduction, and efficient data structures (e.g., KD-trees) to speed up computation.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Euclidean distance measures the straight-line distance between two points in a Cartesian plane, considering the square root of the sum of squared differences along each dimension. Manhattan distance, on the other hand, measures the distance as the sum of absolute differences along each dimension. In KNN, the choice between these distance metrics can affect the results, with Manhattan distance being less sensitive to outliers.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Feature scaling is important in KNN because it ensures that all features contribute equally to the distance calculations. If features have different scales, those with larger scales can dominate the distance metric. Common techniques for feature scaling include min-max scaling (normalization) and z-score standardization. Scaling helps improve the performance and reliability of KNN.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438f645-a6d4-4dee-b163-6389e42cd56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
